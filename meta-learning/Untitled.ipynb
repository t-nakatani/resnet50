{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d487b58b7af8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "\"\"\" Code for loading data. \"\"\"\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "from tensorflow.python.platform import flags\n",
    "from utils import get_images\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "class DataGenerator(object):\n",
    "    \"\"\"\n",
    "    Data Generator capable of generating batches of flower data.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples_per_class, batch_size, config={}):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_samples_per_class: num samples to generate per class in one batch\n",
    "            batch_size: size of meta batch size (e.g. number of functions)\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "\n",
    "        if FLAGS.datasource == 'flower':\n",
    "            self.num_classes = config.get('num_classes', FLAGS.num_classes)\n",
    "            self.num_flowers = config.get('num_flowers', FLAGS.num_flowers)\n",
    "            self.img_size = config.get('img_size', (32, 32))\n",
    "            self.dim_input = np.prod(self.img_size)*3\n",
    "            self.dim_output = self.num_classes\n",
    "            metatrain_folder = config.get('metatrain_folder', './data/flower/train')\n",
    "            if FLAGS.test_set:\n",
    "                metaval_folder = config.get('metaval_folder', './data/flower/test')\n",
    "            else:\n",
    "                metaval_folder = config.get('metaval_folder', './data/flower/val')\n",
    "\n",
    "            metatrain_folders = [os.path.join(metatrain_folder, flower) \\\n",
    "                for flower in os.listdir(metatrain_folder) \\\n",
    "                if os.path.isdir(os.path.join(metatrain_folder, flower)) \\\n",
    "                ]\n",
    "            metaval_folders = [os.path.join(metaval_folder, flower) \\\n",
    "                for flower in os.listdir(metaval_folder) \\\n",
    "                if os.path.isdir(os.path.join(metaval_folder, flower)) \\\n",
    "                ]\n",
    "            self.metatrain_character_folders = metatrain_folders\n",
    "            self.metaval_character_folders = metaval_folders\n",
    "            self.rotations = config.get('rotations', [0])\n",
    "        else:\n",
    "            raise ValueError('Unrecognized data source')\n",
    "\n",
    "\n",
    "    def make_data_tensor(self, train=True):\n",
    "        if train:\n",
    "            folders = self.metatrain_character_folders\n",
    "            # number of tasks, not number of meta-iterations. (divide by metabatch size to measure)\n",
    "            num_total_batches = 200000\n",
    "        else:\n",
    "            folders = self.metaval_character_folders\n",
    "            num_total_batches = 600\n",
    "\n",
    "        # make list of files\n",
    "        print('Generating filenames')\n",
    "        all_filenames = []\n",
    "        for _ in range(num_total_batches):\n",
    "            sampled_character_folders = random.sample(folders, num_flowers)\n",
    "            random.shuffle(sampled_character_folders)\n",
    "            labels_and_images = get_images(sampled_character_folders, range(self.num_flowers), nb_samples=self.num_samples_per_class, shuffle=False)\n",
    "            # make sure the above isn't randomized order\n",
    "            labels = [re.findall('.+_(\\d+).png', li[1])[0] for li in labels_and_images]\n",
    "            filenames = [li[1] for li in labels_and_images]\n",
    "            all_filenames.extend(filenames)\n",
    "\n",
    "        # make queue for tensorflow to read from\n",
    "        filename_queue = tf.train.string_input_producer(tf.convert_to_tensor(all_filenames), shuffle=False)\n",
    "        print('Generating image processing ops')\n",
    "        image_reader = tf.WholeFileReader()\n",
    "        _, image_file = image_reader.read(filename_queue)\n",
    "        if FLAGS.datasource == 'flower':\n",
    "            image = tf.image.decode_png(image_file, channels=3)\n",
    "            image.set_shape((self.img_size[0],self.img_size[1],3))\n",
    "            image = tf.reshape(image, [self.dim_input])\n",
    "            image = tf.cast(image, tf.float32) / 255.0\n",
    "\n",
    "        num_preprocess_threads = 1 # TODO - enable this to be set to >1\n",
    "        min_queue_examples = 256\n",
    "        examples_per_batch = self.num_classes * self.num_samples_per_class\n",
    "        batch_image_size = self.batch_size  * examples_per_batch\n",
    "        print('Batching images')\n",
    "        images = tf.train.batch([image], batch_size = batch_image_size, num_threads=num_preprocess_threads,\n",
    "                capacity=min_queue_examples + 3 * batch_image_size,)\n",
    "        all_image_batches, all_label_batches = [], []\n",
    "        print('Manipulating image data to be right shape')\n",
    "        for i in range(self.batch_size):\n",
    "            image_batch = images[i*examples_per_batch:(i+1)*examples_per_batch]\n",
    "\n",
    "            label_batch = tf.convert_to_tensor(labels)\n",
    "            new_list, new_label_list = [], []\n",
    "            for k in range(self.num_samples_per_class):\n",
    "                class_idxs = tf.range(0, self.num_classes)\n",
    "                class_idxs = tf.random_shuffle(class_idxs)\n",
    "\n",
    "                true_idxs = class_idxs*self.num_samples_per_class + k\n",
    "                new_list.append(tf.gather(image_batch,true_idxs))\n",
    "                new_label_list.append(tf.gather(label_batch, true_idxs))\n",
    "            new_list = tf.concat(new_list, 0)  # has shape [self.num_classes*self.num_samples_per_class, self.dim_input]\n",
    "            new_label_list = tf.concat(new_label_list, 0)\n",
    "            all_image_batches.append(new_list)\n",
    "            all_label_batches.append(new_label_list)\n",
    "        all_image_batches = tf.stack(all_image_batches)\n",
    "        all_label_batches = tf.stack(all_label_batches)\n",
    "        all_label_batches = tf.one_hot(all_label_batches, self.num_classes)\n",
    "        return all_image_batches, all_label_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall('.+_(\\d+).png', li[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Code for loading data. \"\"\"\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.platform import flags\n",
    "from utils import get_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = flags.FLAGS\n",
    "\n",
    "class DataGenerator(object):\n",
    "    \"\"\"\n",
    "    Data Generator capable of generating batches of sinusoid or Omniglot data.\n",
    "    A \"class\" is considered a class of omniglot digits or a particular sinusoid function.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, num_samples_per_class, batch_size, config={}):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_samples_per_class: num samples to generate per class in one batch\n",
    "            batch_size: size of meta batch size (e.g. number of functions)\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.num_classes = 1  # by default 1 (only relevant for classification problems)\n",
    "\n",
    "        if FLAGS.datasource == 'miniimagenet':\n",
    "            self.num_classes = config.get('num_classes', FLAGS.num_classes)\n",
    "            self.img_size = config.get('img_size', (84, 84))\n",
    "            self.dim_input = np.prod(self.img_size)*3\n",
    "            self.dim_output = self.num_classes\n",
    "            metatrain_folder = config.get('metatrain_folder', './data/miniImagenet/train')\n",
    "            if FLAGS.test_set:\n",
    "                metaval_folder = config.get('metaval_folder', './data/miniImagenet/test')\n",
    "            else:\n",
    "                metaval_folder = config.get('metaval_folder', './data/miniImagenet/val')\n",
    "\n",
    "            metatrain_folders = [os.path.join(metatrain_folder, label) \\\n",
    "                for label in os.listdir(metatrain_folder) \\\n",
    "                if os.path.isdir(os.path.join(metatrain_folder, label)) \\\n",
    "                ]\n",
    "            metaval_folders = [os.path.join(metaval_folder, label) \\\n",
    "                for label in os.listdir(metaval_folder) \\\n",
    "                if os.path.isdir(os.path.join(metaval_folder, label)) \\\n",
    "                ]\n",
    "            self.metatrain_character_folders = metatrain_folders\n",
    "            self.metaval_character_folders = metaval_folders\n",
    "            self.rotations = config.get('rotations', [0])\n",
    "        else:\n",
    "            raise ValueError('Unrecognized data source')\n",
    "\n",
    "\n",
    "    def make_data_tensor(self, train=True):\n",
    "        if train:\n",
    "            folders = self.metatrain_character_folders\n",
    "            # number of tasks, not number of meta-iterations. (divide by metabatch size to measure)\n",
    "            num_total_batches = 200000\n",
    "        else:\n",
    "            folders = self.metaval_character_folders\n",
    "            num_total_batches = 600\n",
    "\n",
    "        # make list of files\n",
    "        print('Generating filenames')\n",
    "        all_filenames = []\n",
    "        for _ in range(num_total_batches):\n",
    "            sampled_character_folders = random.sample(folders, self.num_classes)\n",
    "            random.shuffle(sampled_character_folders)\n",
    "            labels_and_images = get_images(sampled_character_folders, range(self.num_classes), nb_samples=self.num_samples_per_class, shuffle=False)\n",
    "            # make sure the above isn't randomized order\n",
    "            labels = [li[0] for li in labels_and_images]\n",
    "            filenames = [li[1] for li in labels_and_images]\n",
    "            all_filenames.extend(filenames)\n",
    "\n",
    "        # make queue for tensorflow to read from\n",
    "        filename_queue = tf.train.string_input_producer(tf.convert_to_tensor(all_filenames), shuffle=False)\n",
    "        print('Generating image processing ops')\n",
    "        image_reader = tf.WholeFileReader()\n",
    "        _, image_file = image_reader.read(filename_queue)\n",
    "        if FLAGS.datasource == 'miniimagenet':\n",
    "            image = tf.image.decode_jpeg(image_file, channels=3)\n",
    "            image.set_shape((self.img_size[0],self.img_size[1],3))\n",
    "            image = tf.reshape(image, [self.dim_input])\n",
    "            image = tf.cast(image, tf.float32) / 255.0\n",
    "\n",
    "        num_preprocess_threads = 1 # TODO - enable this to be set to >1\n",
    "        min_queue_examples = 256\n",
    "        examples_per_batch = self.num_classes * self.num_samples_per_class\n",
    "        batch_image_size = self.batch_size  * examples_per_batch\n",
    "        print('Batching images')\n",
    "        images = tf.train.batch(\n",
    "                [image],\n",
    "                batch_size = batch_image_size,\n",
    "                num_threads=num_preprocess_threads,\n",
    "                capacity=min_queue_examples + 3 * batch_image_size,\n",
    "                )\n",
    "        all_image_batches, all_label_batches = [], []\n",
    "        print('Manipulating image data to be right shape')\n",
    "        for i in range(self.batch_size):\n",
    "            image_batch = images[i*examples_per_batch:(i+1)*examples_per_batch]\n",
    "\n",
    "            label_batch = tf.convert_to_tensor(labels)\n",
    "            new_list, new_label_list = [], []\n",
    "            for k in range(self.num_samples_per_class):\n",
    "                class_idxs = tf.range(0, self.num_classes)\n",
    "                class_idxs = tf.random_shuffle(class_idxs)\n",
    "\n",
    "                true_idxs = class_idxs*self.num_samples_per_class + k\n",
    "                new_list.append(tf.gather(image_batch,true_idxs))\n",
    "                new_label_list.append(tf.gather(label_batch, true_idxs))\n",
    "            new_list = tf.concat(new_list, 0)  # has shape [self.num_classes*self.num_samples_per_class, self.dim_input]\n",
    "            new_label_list = tf.concat(new_label_list, 0)\n",
    "            all_image_batches.append(new_list)\n",
    "            all_label_batches.append(new_label_list)\n",
    "        all_image_batches = tf.stack(all_image_batches)\n",
    "        all_label_batches = tf.stack(all_label_batches)\n",
    "        all_label_batches = tf.one_hot(all_label_batches, self.num_classes)\n",
    "        return all_image_batches, all_label_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Usage Instructions:\n",
    "    10-shot sinusoid:\n",
    "        python main.py --datasource=sinusoid --logdir=logs/sine/ --metatrain_iterations=70000 --norm=None --update_batch_size=10\n",
    "\n",
    "    10-shot sinusoid baselines:\n",
    "        python main.py --datasource=sinusoid --logdir=logs/sine/ --pretrain_iterations=70000 --metatrain_iterations=0 --norm=None --update_batch_size=10 --baseline=oracle\n",
    "        python main.py --datasource=sinusoid --logdir=logs/sine/ --pretrain_iterations=70000 --metatrain_iterations=0 --norm=None --update_batch_size=10\n",
    "\n",
    "    5-way, 1-shot omniglot:\n",
    "        python main.py --datasource=omniglot --metatrain_iterations=60000 --meta_batch_size=32 --update_batch_size=1 --update_lr=0.4 --num_updates=1 --logdir=logs/omniglot5way/\n",
    "\n",
    "    20-way, 1-shot omniglot:\n",
    "        python main.py --datasource=omniglot --metatrain_iterations=60000 --meta_batch_size=16 --update_batch_size=1 --num_classes=20 --update_lr=0.1 --num_updates=5 --logdir=logs/omniglot20way/\n",
    "\n",
    "    5-way 1-shot mini imagenet:\n",
    "        python main.py --datasource=miniimagenet --metatrain_iterations=60000 --meta_batch_size=4 --update_batch_size=1 --update_lr=0.01 --num_updates=5 --num_classes=5 --logdir=logs/miniimagenet1shot/ --num_filters=32 --max_pool=True\n",
    "\n",
    "    5-way 5-shot mini imagenet:\n",
    "        python main.py --datasource=miniimagenet --metatrain_iterations=60000 --meta_batch_size=4 --update_batch_size=5 --update_lr=0.01 --num_updates=5 --num_classes=5 --logdir=logs/miniimagenet5shot/ --num_filters=32 --max_pool=True\n",
    "\n",
    "    To run evaluation, use the '--train=False' flag and the '--test_set=True' flag to use the test set.\n",
    "\n",
    "    For omniglot and miniimagenet training, acquire the dataset online, put it in the correspoding data directory, and see the python script instructions in that directory to preprocess the data.\n",
    "\n",
    "    Note that better sinusoid results can be achieved by using a larger network.\n",
    "\"\"\"\n",
    "import csv\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_generator import DataGenerator\n",
    "from maml import MAML\n",
    "from tensorflow.python.platform import flags\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset/method options\n",
    "flags.DEFINE_string('datasource', 'sinusoid', 'sinusoid or omniglot or miniimagenet')\n",
    "flags.DEFINE_integer('num_classes', 5, 'number of classes used in classification (e.g. 5-way classification).')\n",
    "# oracle means task id is input (only suitable for sinusoid)\n",
    "flags.DEFINE_string('baseline', None, 'oracle, or None')\n",
    "\n",
    "## Training options\n",
    "flags.DEFINE_integer('pretrain_iterations', 0, 'number of pre-training iterations.')\n",
    "flags.DEFINE_integer('metatrain_iterations', 15000, 'number of metatraining iterations.') # 15k for omniglot, 50k for sinusoid\n",
    "flags.DEFINE_integer('meta_batch_size', 25, 'number of tasks sampled per meta-update')\n",
    "flags.DEFINE_float('meta_lr', 0.001, 'the base learning rate of the generator')\n",
    "flags.DEFINE_integer('update_batch_size', 5, 'number of examples used for inner gradient update (K for K-shot learning).')\n",
    "flags.DEFINE_float('update_lr', 1e-3, 'step size alpha for inner gradient update.') # 0.1 for omniglot\n",
    "flags.DEFINE_integer('num_updates', 1, 'number of inner gradient updates during training.')\n",
    "\n",
    "## Model options\n",
    "flags.DEFINE_string('norm', 'batch_norm', 'batch_norm, layer_norm, or None')\n",
    "flags.DEFINE_integer('num_filters', 64, 'number of filters for conv nets -- 32 for miniimagenet, 64 for omiglot.')\n",
    "flags.DEFINE_bool('conv', True, 'whether or not to use a convolutional network, only applicable in some cases')\n",
    "flags.DEFINE_bool('max_pool', False, 'Whether or not to use max pooling rather than strided convolutions')\n",
    "flags.DEFINE_bool('stop_grad', False, 'if True, do not use second derivatives in meta-optimization (for speed)')\n",
    "\n",
    "## Logging, saving, and testing options\n",
    "flags.DEFINE_bool('log', True, 'if false, do not log summaries, for debugging code.')\n",
    "flags.DEFINE_string('logdir', '/tmp/data', 'directory for summaries and checkpoints.')\n",
    "flags.DEFINE_bool('resume', True, 'resume training if there is a model available')\n",
    "flags.DEFINE_bool('train', True, 'True to train, False to test.')\n",
    "flags.DEFINE_integer('test_iter', -1, 'iteration to load model (-1 for latest model)')\n",
    "flags.DEFINE_bool('test_set', False, 'Set to true to test on the the test set, False for the validation set.')\n",
    "flags.DEFINE_integer('train_update_batch_size', -1, 'number of examples used for gradient update during training (use if you want to test with a different number).')\n",
    "flags.DEFINE_float('train_update_lr', -1, 'value of inner gradient step step during training. (use if you want to test with a different value)') # 0.1 for omniglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, saver, sess, exp_string, data_generator, resume_itr=0):\n",
    "    SUMMARY_INTERVAL = 100\n",
    "    SAVE_INTERVAL = 1000\n",
    "\n",
    "    PRINT_INTERVAL = 100\n",
    "    TEST_PRINT_INTERVAL = PRINT_INTERVAL*5\n",
    "\n",
    "    if FLAGS.log:\n",
    "        train_writer = tf.summary.FileWriter(FLAGS.logdir + '/' + exp_string, sess.graph)\n",
    "    print('Done initializing, starting training.')\n",
    "    prelosses, postlosses = [], []\n",
    "    num_classes = data_generator.num_classes # for classification, 1 otherwise\n",
    "    multitask_weights, reg_weights = [], []\n",
    "    for itr in range(resume_itr, FLAGS.pretrain_iterations + FLAGS.metatrain_iterations):\n",
    "        feed_dict = {}\n",
    "        \n",
    "        if itr < FLAGS.pretrain_iterations:\n",
    "            input_tensors = [model.pretrain_op]\n",
    "        else:\n",
    "            input_tensors = [model.metatrain_op]\n",
    "\n",
    "        if (itr % SUMMARY_INTERVAL == 0 or itr % PRINT_INTERVAL == 0):\n",
    "            input_tensors.extend([model.summ_op, model.total_loss1, model.total_losses2[FLAGS.num_updates-1]])\n",
    "            if model.classification:\n",
    "                input_tensors.extend([model.total_accuracy1, model.total_accuracies2[FLAGS.num_updates-1]])\n",
    "\n",
    "        result = sess.run(input_tensors, feed_dict)\n",
    "\n",
    "        if itr % SUMMARY_INTERVAL == 0:\n",
    "            prelosses.append(result[-2])\n",
    "            if FLAGS.log:\n",
    "                train_writer.add_summary(result[1], itr)\n",
    "            postlosses.append(result[-1])\n",
    "\n",
    "        if (itr!=0) and itr % PRINT_INTERVAL == 0:\n",
    "            if itr < FLAGS.pretrain_iterations:\n",
    "                print_str = 'Pretrain Iteration ' + str(itr)\n",
    "            else:\n",
    "                print_str = 'Iteration ' + str(itr - FLAGS.pretrain_iterations)\n",
    "            print_str += ': ' + str(np.mean(prelosses)) + ', ' + str(np.mean(postlosses))\n",
    "            print(print_str)\n",
    "            prelosses, postlosses = [], []\n",
    "\n",
    "        if (itr!=0) and itr % SAVE_INTERVAL == 0:\n",
    "            saver.save(sess, FLAGS.logdir + '/' + exp_string + '/model' + str(itr))\n",
    "\n",
    "        # sinusoid is infinite data, so no need to test on meta-validation set.\n",
    "        if (itr!=0) and itr % TEST_PRINT_INTERVAL == 0 and FLAGS.datasource !='sinusoid':\n",
    "            if 'generate' not in dir(data_generator):\n",
    "                feed_dict = {}\n",
    "                if model.classification:\n",
    "                    input_tensors = [model.metaval_total_accuracy1, model.metaval_total_accuracies2[FLAGS.num_updates-1], model.summ_op]\n",
    "                else:\n",
    "                    input_tensors = [model.metaval_total_loss1, model.metaval_total_losses2[FLAGS.num_updates-1], model.summ_op]\n",
    "\n",
    "            result = sess.run(input_tensors, feed_dict)\n",
    "            print('Validation results: ' + str(result[0]) + ', ' + str(result[1]))\n",
    "\n",
    "    saver.save(sess, FLAGS.logdir + '/' + exp_string +  '/model' + str(itr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculated for omniglot\n",
    "NUM_TEST_POINTS = 600\n",
    "\n",
    "def test(model, saver, sess, exp_string, data_generator, test_num_updates=None):\n",
    "    num_classes = data_generator.num_classes # for classification, 1 otherwise\n",
    "\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "\n",
    "    metaval_accuracies = []\n",
    "\n",
    "    for _ in range(NUM_TEST_POINTS):\n",
    "        if 'generate' not in dir(data_generator):\n",
    "            feed_dict = {}\n",
    "            feed_dict = {model.meta_lr : 0.0}\n",
    "\n",
    "        if model.classification:\n",
    "            result = sess.run([model.metaval_total_accuracy1] + model.metaval_total_accuracies2, feed_dict)\n",
    "    metaval_accuracies = np.array(metaval_accuracies)\n",
    "    means = np.mean(metaval_accuracies, 0)\n",
    "    stds = np.std(metaval_accuracies, 0)\n",
    "    ci95 = 1.96*stds/np.sqrt(NUM_TEST_POINTS)\n",
    "\n",
    "    print('Mean validation accuracy/loss, stddev, and confidence intervals')\n",
    "    print((means, stds, ci95))\n",
    "\n",
    "    out_filename = FLAGS.logdir +'/'+ exp_string + '/' + 'test_ubs' + str(FLAGS.update_batch_size) + '_stepsize' + str(FLAGS.update_lr) + '.csv'\n",
    "    out_pkl = FLAGS.logdir +'/'+ exp_string + '/' + 'test_ubs' + str(FLAGS.update_batch_size) + '_stepsize' + str(FLAGS.update_lr) + '.pkl'\n",
    "    with open(out_pkl, 'wb') as f:\n",
    "        pickle.dump({'mses': metaval_accuracies}, f)\n",
    "    with open(out_filename, 'w') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow(['update'+str(i) for i in range(len(means))])\n",
    "        writer.writerow(means)\n",
    "        writer.writerow(stds)\n",
    "        writer.writerow(ci95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if:\n",
    "        if FLAGS.datasource == 'miniimagenet':\n",
    "            if FLAGS.train == True:\n",
    "                test_num_updates = 1  # eval on at least one update during training\n",
    "            else:\n",
    "                test_num_updates = 10\n",
    "        else:\n",
    "            test_num_updates = 10\n",
    "\n",
    "    if FLAGS.train == False:\n",
    "        orig_meta_batch_size = FLAGS.meta_batch_size\n",
    "        # always use meta batch size of 1 when testing.\n",
    "        FLAGS.meta_batch_size = 1\n",
    "\n",
    "    if FLAGS.datasource != 'sinusoid':\n",
    "        if FLAGS.metatrain_iterations == 0 and FLAGS.datasource == 'miniimagenet':\n",
    "            assert FLAGS.meta_batch_size == 1\n",
    "            assert FLAGS.update_batch_size == 1\n",
    "            data_generator = DataGenerator(1, FLAGS.meta_batch_size)  # only use one datapoint,\n",
    "        else:\n",
    "            if FLAGS.datasource == 'miniimagenet': # TODO - use 15 val examples for imagenet?\n",
    "                if FLAGS.train:\n",
    "                    data_generator = DataGenerator(FLAGS.update_batch_size+15, FLAGS.meta_batch_size)  # only use one datapoint for testing to save memory\n",
    "                else:\n",
    "                    data_generator = DataGenerator(FLAGS.update_batch_size*2, FLAGS.meta_batch_size)  # only use one datapoint for testing to save memory\n",
    "\n",
    "    dim_output = data_generator.dim_output\n",
    "    if FLAGS.baseline == 'oracle':\n",
    "        assert FLAGS.datasource == 'sinusoid'\n",
    "        dim_input = 3\n",
    "        FLAGS.pretrain_iterations += FLAGS.metatrain_iterations\n",
    "        FLAGS.metatrain_iterations = 0\n",
    "    else:\n",
    "        dim_input = data_generator.dim_input\n",
    "\n",
    "    if FLAGS.datasource == 'miniimagenet' or FLAGS.datasource == 'omniglot':\n",
    "        tf_data_load = True\n",
    "        num_classes = data_generator.num_classes\n",
    "\n",
    "        if FLAGS.train: # only construct training model if needed\n",
    "            random.seed(5)\n",
    "            image_tensor, label_tensor = data_generator.make_data_tensor()\n",
    "            inputa = tf.slice(image_tensor, [0,0,0], [-1,num_classes*FLAGS.update_batch_size, -1])\n",
    "            inputb = tf.slice(image_tensor, [0,num_classes*FLAGS.update_batch_size, 0], [-1,-1,-1])\n",
    "            labela = tf.slice(label_tensor, [0,0,0], [-1,num_classes*FLAGS.update_batch_size, -1])\n",
    "            labelb = tf.slice(label_tensor, [0,num_classes*FLAGS.update_batch_size, 0], [-1,-1,-1])\n",
    "            input_tensors = {'inputa': inputa, 'inputb': inputb, 'labela': labela, 'labelb': labelb}\n",
    "\n",
    "        random.seed(6)\n",
    "        image_tensor, label_tensor = data_generator.make_data_tensor(train=False)\n",
    "        inputa = tf.slice(image_tensor, [0,0,0], [-1,num_classes*FLAGS.update_batch_size, -1])\n",
    "        inputb = tf.slice(image_tensor, [0,num_classes*FLAGS.update_batch_size, 0], [-1,-1,-1])\n",
    "        labela = tf.slice(label_tensor, [0,0,0], [-1,num_classes*FLAGS.update_batch_size, -1])\n",
    "        labelb = tf.slice(label_tensor, [0,num_classes*FLAGS.update_batch_size, 0], [-1,-1,-1])\n",
    "        metaval_input_tensors = {'inputa': inputa, 'inputb': inputb, 'labela': labela, 'labelb': labelb}\n",
    "    else:\n",
    "        tf_data_load = False\n",
    "        input_tensors = None\n",
    "\n",
    "    model = MAML(dim_input, dim_output, test_num_updates=test_num_updates)\n",
    "    if FLAGS.train or not tf_data_load:\n",
    "        model.construct_model(input_tensors=input_tensors, prefix='metatrain_')\n",
    "    if tf_data_load:\n",
    "        model.construct_model(input_tensors=metaval_input_tensors, prefix='metaval_')\n",
    "    model.summ_op = tf.summary.merge_all()\n",
    "\n",
    "    saver = loader = tf.train.Saver(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES), max_to_keep=10)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    if FLAGS.train == False:\n",
    "        # change to original meta batch size when loading model.\n",
    "        FLAGS.meta_batch_size = orig_meta_batch_size\n",
    "\n",
    "    if FLAGS.train_update_batch_size == -1:\n",
    "        FLAGS.train_update_batch_size = FLAGS.update_batch_size\n",
    "    if FLAGS.train_update_lr == -1:\n",
    "        FLAGS.train_update_lr = FLAGS.update_lr\n",
    "\n",
    "    exp_string = 'cls_'+str(FLAGS.num_classes)+'.mbs_'+str(FLAGS.meta_batch_size) + '.ubs_' + str(FLAGS.train_update_batch_size) + '.numstep' + str(FLAGS.num_updates) + '.updatelr' + str(FLAGS.train_update_lr)\n",
    "\n",
    "    if FLAGS.num_filters != 64:\n",
    "        exp_string += 'hidden' + str(FLAGS.num_filters)\n",
    "    if FLAGS.max_pool:\n",
    "        exp_string += 'maxpool'\n",
    "    if FLAGS.stop_grad:\n",
    "        exp_string += 'stopgrad'\n",
    "    if FLAGS.baseline:\n",
    "        exp_string += FLAGS.baseline\n",
    "    if FLAGS.norm == 'batch_norm':\n",
    "        exp_string += 'batchnorm'\n",
    "    elif FLAGS.norm == 'layer_norm':\n",
    "        exp_string += 'layernorm'\n",
    "    elif FLAGS.norm == 'None':\n",
    "        exp_string += 'nonorm'\n",
    "    else:\n",
    "        print('Norm setting not recognized.')\n",
    "\n",
    "    resume_itr = 0\n",
    "    model_file = None\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    tf.train.start_queue_runners()\n",
    "\n",
    "    if FLAGS.resume or not FLAGS.train:\n",
    "        model_file = tf.train.latest_checkpoint(FLAGS.logdir + '/' + exp_string)\n",
    "        if FLAGS.test_iter > 0:\n",
    "            model_file = model_file[:model_file.index('model')] + 'model' + str(FLAGS.test_iter)\n",
    "        if model_file:\n",
    "            ind1 = model_file.index('model')\n",
    "            resume_itr = int(model_file[ind1+5:])\n",
    "            print(\"Restoring model weights from \" + model_file)\n",
    "            saver.restore(sess, model_file)\n",
    "\n",
    "    if FLAGS.train:\n",
    "        train(model, saver, sess, exp_string, data_generator, resume_itr)\n",
    "    else:\n",
    "        test(model, saver, sess, exp_string, data_generator, test_num_updates)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
